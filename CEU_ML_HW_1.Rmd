---
title: "Machine Learning -  Homework 1"
author: "Tamas Koncz"
date: '2018-01-28'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

```{r, message=FALSE}
library(data.table)
library(ggplot2)
library(caret)
library(rpart)
library(titanic)

theme_set(theme_bw())   # globally set ggplot theme

set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
```


### 1. Model selection with a validation set (5 points)

```{r}

# data <- fread("../../data/king_county_house_prices/kc_house_data.csv")
data <- fread("kc_house_data.csv")

data[, `:=`(floors = as.numeric(floors), zipcode = factor(zipcode))]
data[, log_price := log(price)]

data[, c("id", "date", "sqft_living15", "sqft_lot15", "price") := NULL]

set.seed(1234)
```

#### a. Using createDataPartition, cut your data into three parts: 50% should be your training data, 25% each your validation and test sets (hint: cut data into two parts, then further cut one part into two).

```{r}
training_ratio <- 0.5
set.seed(1234)
train_indices <- createDataPartition(y = data[["log_price"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_temp <- data[-train_indices, ]
set.seed(1234)
validation_indices <- createDataPartition(y = data_temp[["log_price"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_validation <- data_temp[validation_indices, ]
data_test <- data_temp[-validation_indices, ]
rm(data_temp)
```


#### b. Train three models on the training data via caret, without cross validation (method = "none"):
```{r}
train_control <- trainControl(method = "none")
```

####    1. a linear model lm with only using sqft_living as a predictor (a simple benchmark)

```{r}
set.seed(1234)
model_simple_linear <- train(log_price ~ sqft_living, 
                     data = data_train, 
                     method = "lm", 
                     trControl = train_control)
```

####    2. a linear model lm using all available features

```{r}
set.seed(1234)
model_linear <- train(log_price ~ ., 
                   data = data_train, 
                   method = "lm", 
                   trControl = train_control)
```

####    3. a regression tree (rpart) with cp = 0.0001 (the tune grid should be a dataframe with one column cp and one row with value 0.0001)

```{r}
tune_grid <- data.frame("cp" = 0.0001)
set.seed(1234)
model_rpart <- train(log_price ~ ., 
                   data = data_train, 
                   method = "rpart", 
                   trControl = train_control,
                   tuneGrid = tune_grid)
```

#### c. Compare your models on the validation set and choose the one with the best performance (using RMSE). Use predict.train for prediction just like we used predict in class.

```{r}
simple_linear_rmse <- RMSE(predict.train(model_simple_linear, newdata = data_validation), data_validation[["log_price"]])
linear_rmse <- RMSE(predict.train(model_linear, newdata = data_validation), data_validation[["log_price"]])
rpart_rmse <- RMSE(predict.train(model_rpart, newdata = data_validation), data_validation[["log_price"]])

simple_linear_rmse
linear_rmse
rpart_rmse
```
Based on RMSE on the validation set, the second model, the linear regression containing all variables performed the best.

##### d. Evaluate the final model on the test set. Why is it important to have this final set of observations set aside for evaluation? (Hint: think about what we used the validation set for.)

```{r}
final_performance_measure <- RMSE(predict.train(model_linear, newdata = data_test), data_test[["log_price"]])
final_performance_measure
```
The final model has an error rate of 0.19 on the test set - slightly better than what we measured on the validation set, hence the results can be deemed stable.
The reason to use this third set for testing performance is to avoid any cross-dependence between the data used for evaluation of final performance and model tuning/selection. The training set was used for training the models, hence obviosuly not a good dataset to measure performance. We set aside a validation set, however this can be also misleading - this was used for selecting the best model from the three models trained, hence the results could be "biased" towards the selected one. A third, independent dataset for testing can solve this problem for us.

#### e. Do you think it makes more sense to use this method rather than the one used in class? What can be advantages or disadvantages of one or the other?

The difference is that in class we used a k-fold (k=10, repeated 5 times) CV method, while here we used a simple validation set approach.
The approach used here is clearly more computational-friendly, however it comes at a price: the actual models and estimates may depend on the data-split, and can vary significantly (this problem is less apparent with larger sample sizes).

### 2. Predicting developer salaries (5 points)

#### a) Describe what the data cleansing steps mean.

Explanations below each code piece:
```{r}
#data <- fread("../../data/stackoverflow2017/survey_results_public_selected.csv")
data <- fread("survey_results_public_selected.csv")
```

Reads in data for the .csv file.

```{r}
data <- data[!is.na(Salary) & Salary > 0]
```

Removes rows, where Salary information is missing (null), or it is below / equals to 0.
Dataset goes from 51392 observations to 12885 - however, this is necessary, so that the model can be trained on meaningful salary information. 

```{r}
data <- data[complete.cases(data)]
```

Removes any rows with missing (null) information in any column.
No impact on the dataset in this case. The step could be used to avoid cases where a model can't handle missing information gracefully.

```{r}
data <- data[, Gender := ifelse(Gender == "Male", "Male",
                              ifelse(Gender == "Female", "Female", "Other"))]
```

In the original dataset, there were 20 different gender codes - the above reduces this to three (by bucketing everything that's not either "Male" or "Female" as "Other").

```{r}
large_countries <- data[, .N, by = "Country"][N > 60][["Country"]]
data <- data[, Country := ifelse(Country %in% large_countries, Country, "Other")]
rm(large_countries)
```

This step is similar to the above - however, here countries with 60 or less observations are categorized as "Other".
(Observation types that are very rare might not be much useful for prediction, hence grouping them can help)

#### b) Using graphs, find at least two interesting features that can contribute to understanding developer salaries.

```{r}
dt_size_mapping <- data.table(unique(data$CompanySize))
dt_size_mapping <- cbind(dt_size_mapping, 
                         data.table(c("10K<", "5K<", "1K<", 
                                 "500<", "10<", "100<", 
                                 "20<", "10>", 
                                 "Other", "Other", "Other" )))
                         
setnames(dt_size_mapping, c("CompanySize", "CompSize"))
data <- merge(data, dt_size_mapping, by = "CompanySize", all.x = TRUE)
rm(dt_size_mapping)

data[, CompSize := factor(CompSize, levels = c("10K<", "5K<", "1K<", "500<", "100<", "20<", "10<", "10>", "Other"))]
```

The above piece of code remaps the "CompanySize" variable to a more readable format for charts, as well as sets the right order level.

```{r, fig.width=15, fig.height=7}
#data[, .(count = .N, avg = mean(Salary)), by = .(CompanySize, CompSize)][order(CompSize)]

summary <- data[, 
                .(avg_salary = mean(Salary)), 
                by = .(CompSize, Gender)][order(CompSize, Gender)]

ggplot() + 
      geom_point(data = data, aes(x = CompSize, y = Salary / 1000), color = "blue", alpha = 0.1) + 
      geom_line(data = summary, aes(x = CompSize, y = avg_salary / 1000), color = "orange", size = 1.25, group = 1) +
      geom_text(data = summary, aes(x = CompSize, y = avg_salary / 1000, label = round(avg_salary / 1000)), color = "orange", group = 1, hjust = 0, vjust = 0) +
      facet_grid(~Gender) +
      labs(title = "Salary comparison across company sizes", 
           subtitle = "Orange line: average salary. Blue dots are individuals",
           x = "Company size",
           y = "Salary",
           caption = "Numbers are in $K")

```

We can observe at least three important things on the above chart:

      1. Generally, all genders are getting better paid the larger the firm size
      2. Visually, the gender gap does not seem significant: all three genders are getting paid similar amount, with similar dynamics (see above point)
      3. There are a lot more males among the observations than either females or "other"

```{r, fig.width=15, fig.height=7}
temp <- data[, 
                .(avg_salary = mean(Salary)), 
                by = .(Country, Gender)]
temp2 <- data[, 
                .(avg_salary = mean(Salary)), 
                by = .(Country)]

summary2 <- temp[, Country := factor(Country, levels = temp2[order(avg_salary)][, Country])]

ggplot(data = summary2, aes(x = Country, y = avg_salary / 1000)) + 
      geom_bar(stat = "identity", fill = "blue") + coord_flip() +
      geom_text(aes(x = Country, y = avg_salary / 1000, label = round(avg_salary / 1000)), color = "black", group = 1, hjust = 0) +
      facet_grid(~Gender) +
      labs(title = "Salary comparison across countries",
           x = "Country",
           y = "Salary",
           caption = "Numbers are in $K")

## I had to leave out the TabsSpaces column, as I really didn't like the result :( # https://www.youtube.com/watch?v=SsoOG6ZeyUI
```

Let's highlight two important (although not suprising) observations:

      1. Country is a key factor in determining the salary level
      2. Most countries actually show a gender gap in mean pay - we did not see such a thing in the breakdown by company size (think number of people working at the different types of companies!)

#### c) Create a training and a test set assigning 70% to the training set and 30% as the test set.

```{r}
data[, log_salary := log(Salary)]

training_ratio <- 0.7
set.seed(1234)
train_indices <- createDataPartition(y = data[["log_salary"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```


#### d) Using caret train at least two predictive models to predict the logarithm of Salary (they can be of the same family but with different hyperparameters or they can be of different families like we used lm and rpart in the first exercise). Make sure NOT to include Salary as a predictor variable. Also, just before calling train, remember to use set.seed.

Training two models, without setting any additional control parameters: 

```{r}

fit_control <- trainControl(method = "cv", number = 10)

set.seed(1234)
rpart_fit <- train(log_salary ~ . -Salary, 
                   data = data_train, 
                   method = "rpart", 
                   trControl = fit_control)
set.seed(1234)
glm_fit <- train(log_salary ~ . -Salary, 
                   data = data_train, 
                   method = "glm", 
                   trControl = fit_control)
```



#### Then:
####    1. choose the best model based on cross-validation estimation on the training set

```{r}
rpart_fit
```

```{r}
glm_fit
```


Based on the RMSE values, even the best tree performed worse than the linear model, so we'll continue with that one.

####    2. evaluate its performance on the test set

```{r}
avg_salary <- mean(data_train$Salary)
RMSE(log(avg_salary), data_test[["log_salary"]])

test_prediction <- predict.train(glm_fit, newdata = data_test)
RMSE(test_prediction, data_test[["log_salary"]])
```

To have some basis for comparison, I've calculated RMSE with using the log(Avg. Salary) as well. As it shows, there is significant improvement compared to this "basic model".
Also, even though the performance is worse on the test set (naturally), it is not a terrible difference compared to the error on the training set.

#### e. Compare the true and predicted values of the test set on a graph. How do you evaluate the model fit based on this graph?

```{r}
eval <- data.table(cbind(actual = data_test$log_salary, predicted = test_prediction))

ggplot(data = eval, aes(x = actual, y = predicted)) + 
  geom_point(alpha = .1) + 
  geom_abline(intercept = log(avg_salary), slope = 0, color = "blue") +
  geom_abline(slope = 1, color = "red") +
  scale_y_continuous(limits = c(-5, 15)) +
  scale_x_continuous(limits = c(-5, 15))  +
      labs(title = "Model performance - Actual vs Predicted values for log(Salary)",
           subtitle = "blue - avg. salary, red - x=y line",
           x = "Actual",
           y = "Predicted")
```
Based on the above, the model does a relatively good job around the most frequent salary range (~intercept of the two lines, the mean salary).
However, its performance is the worse the further we go from this range - this is likely due to the limitation of the linear model.


### 3. Leave-one-out cross validation (3 points)


#### a. Name a disadvantage of this method compared to using a moderate value (say, 10) for k?

Two major ones:
    1. Given the model needs to be fit n times, for a large dataset it can be computationally very expensive (also depending on the model)
    2. LOOCV tends to have higher variance - this is due to the fact that the n models are trained on almost identical data, hence the fitted models demonstrate more significant correlation

#### b. Why do you think it can still make sense to compute this measure? In what way can this measure be closer to the “real” performance of the model?

Going back to point 2. in the above - the flip side of the LOOCV is that it's better at bias reduction (compared to k-fold CV).
K-fold CV might also have more variability in results depending on the actual split - however, this might not be significant in real scenarios.

```{r}
data_train <- data.table(titanic_train)
# recode Survived to factor - needed for binary prediction
data_train[, Survived := factor(ifelse(Survived == 1, "survived", "died"))]
data_train[, .N, by = Survived]
```

#### c. You can implement LOOCV with caret by setting an option in trainControl: method = "loocv". and use a simple logit model glm for prediction.
####    1. In caret, you can use it via method = "glm"
####    2. include classProbs = TRUE in trainControl to let train know that you are predicting a binary outcome

```{r}

fit_control <- trainControl(method = "loocv", classProbs = TRUE)

set.seed(1234)
glm_fit_loocv <- train(Survived ~ Fare + Sex, 
                   data = data_train, 
                   method = "glm", 
                   trControl = fit_control)

fit_control <- trainControl(method = "cv", number = 10, classProbs = TRUE)

set.seed(1234)
glm_fit_kfoldcv <- train(Survived ~ Fare + Sex, 
                   data = data_train, 
                   method = "glm", 
                   trControl = fit_control)
```

#### d. Compare the accuracy of the model estimated by the two resampling methods via summary(fitted_model$resample). Accuracy is the share of cases predicted correctly.
####    1. How large are the means?
```{r}
summary(glm_fit_loocv$resample)
```
```{r}
summary(glm_fit_kfoldcv$resample)
```

Means are almost virtually identical, both models have predicted ~78% of the cases correctly.

####    2. How do other quantiles look like? Why are quantiles of the accuracy measures of LOOCV so extreme (either 0 or 1)?

For the k=10-fold CV, results are nicely distributed in the 72% - 87% range (this is still signficant variability, however looking at the 1st - 3rd quartiles the gap is very close, just 3% difference in hit rate).

For the LOOCV, the extreme results are due to the method itself - for every run, the error rate is measure only on one observation, which is by definition either correctly or not correctly predicted for binary targets.
