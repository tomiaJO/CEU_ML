---
title: "Machine Learning -  Homework 2"
author: "Tamas Koncz"
date: '2018-02-04'
output:
  html_notebook:
    df_print: paged
  html_document:
    df_print: paged
---

```{r, message=FALSE}
library(data.table)
library(ggplot2)
library(gridExtra)
library(caret)
library(rpart)
library(skimr)
library(ROCR)
library(zoo)

options(scipen = 999)

theme_set(theme_bw())   # globally set ggplot theme

set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
```

#### 1. Predicting mental health problems in the tech sector (8 points)

```{r} 
data <- fread("survey_cleaned.csv")

# data <- fread("../../data/mental-health-in-tech/survey_cleaned.csv")

data <- data[ ,c("comments", "state","work_interfere") := NULL]
data[, age := as.numeric(age)]
data[ , treatment := factor(treatment, levels = c("Yes", "No"))]
```

##### a. Explore some predictors that can be used to predict treatment.
 
 
Observing the column descriptions in Kaggle, I decided to use only a subset of the variables for prediction - the reason for this was that with many columns, without further context, it was not possible to decide if they would actually be impacted by the target variable (e.g. I assume that people who have been involved with a medical condition, are much more aware of the related firm policies, etc.) 


```{r}
data[, is_treated := ifelse(treatment == "Yes", 1, 0)]
```

 
Rather than writing long summaries about what we could observe in the data, let the below plots show the important high-level mechanics: 

```{r}
# gender
dt <- data[ ,.(avg = sum(is_treated) / .N), keyby = gender]
p1 <- ggplot(data = dt) + 
  geom_bar(aes(x = gender, y = avg), stat = "identity") + 
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  ylab("Treated % (compared to total)") +
  coord_flip()
```


```{r}
# country
data[, country_cnt := .N, by = country]

data[, country := ifelse(country_cnt >= 20, country, "Other")]

dt2 <- data[ ,.(avg = sum(is_treated) / .N), keyby = country]
dt2 <- dt2[, country := factor(country, levels = dt2[order(-avg)][["country"]])]
p2 <- ggplot(data = dt2) + 
  geom_bar(aes(x = country, y = avg), stat = "identity") + 
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  ylab("Treated % (compared to total)") +
  coord_flip()
```


```{r, fig.width=15, fig.height=6}
grid.arrange(p1, p2, ncol = 2)
```


```{r, fig.width=15, fig.height=6}
# age
data[, age := round(age)]
data <- data[age >= 18]
data <- data[age <= 65]

dt3 <- data[ ,.(avg = sum(is_treated) / .N), keyby = age]
dt3 <- dt3[order(age)][, rollavg := rollmean(avg, 5, fill = NA)]
ggplot() + 
  geom_point(data = data, aes(x = age, y = is_treated, color = "blue"), stat = "identity", alpha = .1)  +
  geom_line(data = dt3, aes(x = age, y = rollavg, color = "orange"), size = 1.5) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(20, 65), breaks = seq(20, 65, 5)) +
  ylab("Treated % (compared to total)") +
  scale_colour_manual(name = 'Legend', 
                      values = c('blue'='blue','orange'='orange'), 
                      labels = c('Individual observations','Five-year rolling average'))

```

```{r}
# family_history
dt4 <- data[ ,.(avg = sum(is_treated) / .N), keyby = family_history]
p4 <- ggplot(data = dt4) + 
  geom_bar(aes(x = family_history, y = avg), stat = "identity") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  ylab("Treated % (compared to total)") +
  coord_flip()
```

```{r}
# self_employed -- > not much differentiating & lot of missing values (those could be likely extrapolated)

dt5 <- data[ ,.(avg = sum(is_treated) / .N), keyby = remote_work]
p5 <- ggplot(data = dt5) + 
  geom_bar(aes(x = remote_work, y = avg), stat = "identity") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  ylab("Treated % (compared to total)") +
  coord_flip()
```

```{r}
data[, no_employees := factor(no_employees, levels = c("1-5", "6-25", "26-100", "100-500", "500-1000", "More than 1000"))]
dt6 <- data[ ,.(avg = sum(is_treated) / .N), keyby = no_employees]
p6 <- ggplot(data = dt6) + 
  geom_bar(aes(x = no_employees, y = avg), stat = "identity") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  ylab("Treated % (compared to total)") +
  coord_flip()
```


```{r, fig.width=15, fig.height=6}
grid.arrange(p4, p5, p6, ncol = 3)
```


Keep only the observations that we'll use for prediction: 

```{r}
data <- data[, c("treatment", "no_employees", "self_employed", 
                  "remote_work", "family_history", "age", "country", 
                  "gender")]
```

 
 
##### b. Partition your data to 70% training and 30% test samples.

```{r}
training_ratio <- 0.7
set.seed(1234)
train_indices <- createDataPartition(y = data[["treatment"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```


##### c. Build models with glmnet and rpart that predict the binary outcome of treatment (you don’t have to use all variables if you don’t want to - experiment! Just use the same variables for both model families). Use cross-validation on the training set and use AUC as a selection measure (use metric = "ROC" in train and also don’t forget to use classProbs = TRUE, summaryFunction = twoClassSummary in  trainControl). Make sure to set the same seed before each call to train.

```{r}
fit_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)

# rpart
set.seed(1234)
rpart_fit <- train(treatment ~ ., 
                   data = data_train, 
                   method = "rpart",
                   metric = "ROC",
                   trControl = fit_control)

# glmnet
set.seed(1234)
glmnet_fit <- train(treatment ~ ., 
                   data = data_train, 
                   method = "glmnet",
                   metric = "ROC",
                   trControl = fit_control)
```


##### d. Compare models based on their predictive performance based on the cross-validation information (you can just use the mean AUC to select the best model).

```{r}
rpart_fit
```

```{r}
glmnet_fit
```

The two models have very similar performances when evaluated by AUC on the training set - however, the best glmnet model somewhat outdoes the best tree model (.69 vs .675 AUC), hence I'll continue with glmnet for evaluation on the test set.

##### e. Evaluate the best model on the test set: draw an ROC curve and calculate and interpret the AUC.

```{r}
# glmnet
test_prediction_glmnet <- predict.train(glmnet_fit, 
                                        newdata = data_test, 
                                        type = "prob")

glmnet_prediction <- prediction(test_prediction_glmnet$Yes,
                                data_test[["treatment"]])
glmnet_perf <- performance(glmnet_prediction, measure = "tpr", x.measure = "fpr")

glmnet_roc_df <- data.table(
  model = "glmnet",
  FPR = glmnet_perf@x.values[[1]],
  TPR = glmnet_perf@y.values[[1]],
  cutoff = glmnet_perf@alpha.values[[1]]
)

ggplot(glmnet_roc_df) +
  geom_line(aes(FPR, TPR, color = model), size = 2) +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  xlab("False Positive Rate") + ylab("True Positive Rate") 
```

```{r}
# calculate AUC
glmnet_AUC <- performance(glmnet_prediction, measure = "auc")@y.values[[1]]
glmnet_AUC
```
If we were to take a random positive and a random negative observation, the model is expected to assign a higher probability to the "treated" case in ~71% of the time.

The model does fairly well at picking up true positives - however, AUC in itself does not tell much about performance (higher is better obviosuly, but what is actually good is very domain specific).

##### If you have to choose a probability threshold to predict the outcome, what would you choose? At this threshold, how large are the true positive rate and the false positive rate? How many false positives and false negatives there are in the test sample?
 
Selecting a good threshold depends on the actual use case. Let's say that we are after predicting people who should seek medical help in our organisation - missing someone puts them in a wrong spot, but spending too many people to the doctor could be a significant burden. Hence, our preferences are symmetric for false positives and false negatives.

```{r, fig.align='center'}
# built-in plot method
plot(performance(glmnet_prediction, "tpr", "fpr"), colorize=TRUE) 
```


In this case, I'd choose a cutoff beteen ~0.45 - 0.55 (there is no significant performance difference in this range). The TPR is around 0.6, while the FPR is below 0.2, a well-balanced trade-off for our case.

```{r}
test_prediction <- ifelse(test_prediction_glmnet$Yes >= 0.50, "Yes", "No")
test_prediction <- factor(test_prediction, levels = c("Yes", "No"))
confusionMatrix(test_prediction, data_test[["treatment"]])
```

Based on the confusion matrix, the number of false positives is 23, while the number of false negatives is 96 in the test sample. 


#### 2. Transformed scores (5 points)

```{r}
data <- fread("no-show-data.csv")
# data <- fread("../../data/medical-appointments-no-show/no-show-data.csv")
```

```{r}
# [... apply the cleaning steps we did in class ...]

data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]
setnames(data, 
         c("No-show", 
           "Age", 
           "Gender",
           "ScheduledDay", 
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"), 
         c("no_show", 
           "age", 
           "gender", 
           "scheduled_day", 
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))
# clean up a little bit
data <- data[age %between% c(0, 95)]
# for binary prediction with caret, the target variable must be a factor
data[, no_show := factor(no_show, levels = c("Yes", "No"))]
data[, no_show_num := ifelse(no_show == "Yes", 1, 0)]
data[, handicap := ifelse(handicap > 0, 1, 0)]

# create new variables
data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]
data <- data

data[, no_show_num := NULL]

data[, days_category := cut(
  days_since_scheduled, 
  breaks = c(-1, 0, 1, 2, 5, 10, 30, Inf), 
  include.lowest = TRUE)]

data[, age_category := cut(age, 
                           breaks = seq(0, 100, by = 5), 
                           include.lowest = TRUE)]

data <- data[complete.cases(data)]
```

```{r}
# [... create train and test sets ... ]

training_ratio <- 0.5 
set.seed(1234)
train_indices <- createDataPartition(y = data[["no_show"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

```{r}
train_control <- trainControl(method = "cv",
                              number = 5,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

set.seed(1234)
model <- train(no_show ~ days_category + 
                                poly(age, 3) +
                                scholarship +
                                gender +
                                alcoholism +
                                diabetes,
                      data = data_train,
                      method = "glmnet",
                      trControl = train_control,
                      metric = "ROC")  
```


```{r}
prediction <- predict.train(model, newdata = data_test, type = "prob")
prediction_sqrt <- sqrt(prediction)
prediction_sq <- data.frame(prediction^2)
```

#### a. Draw ROC curves for all three scores and calculate the AUC. How do they compare? Is it surprising in light of the interpretation of the AUC?

```{r}
# simple
simple_prediction <- prediction(prediction$Yes, data_test[["no_show"]])
simple_perf <- performance(simple_prediction, measure = "tpr", x.measure = "fpr")

simple_roc_df <- data.table(
  prediction = "simple",
  FPR = simple_perf@x.values[[1]],
  TPR = simple_perf@y.values[[1]],
  cutoff = simple_perf@alpha.values[[1]]
)

# sqrt
sqrt_prediction <- prediction(prediction_sqrt$Yes, data_test[["no_show"]])
sqrt_perf <- performance(sqrt_prediction, measure = "tpr", x.measure = "fpr")

sqrt_roc_df <- data.table(
  prediction = "sqrt",
  FPR = sqrt_perf@x.values[[1]],
  TPR = sqrt_perf@y.values[[1]],
  cutoff = sqrt_perf@alpha.values[[1]]
)

# sq
sq_prediction <- prediction(prediction_sq$Yes, data_test[["no_show"]])
sq_perf <- performance(sq_prediction, measure = "tpr", x.measure = "fpr")

sq_roc_df <- data.table(
  prediction = "sq",
  FPR = sq_perf@x.values[[1]],
  TPR = sq_perf@y.values[[1]],
  cutoff = sq_perf@alpha.values[[1]]
)
```

```{r}
roc_df <- rbind(simple_roc_df, sqrt_roc_df)
roc_df <- rbind(roc_df, sq_roc_df)
```

```{r}
ggplot(roc_df) +
  geom_line(aes(FPR, TPR, color = prediction), size = 2) +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  xlab("False Positive Rate") + ylab("True Positive Rate")
```



#### b. What is the key, common property of both the square root and the square functions that leads to this finding?

#### c. Draw a calibration plot for all three scores separately:
####      1. group people into bins based on predicted scores
####      2. display on a scatterplot the mean of the predicted scores versus the actual share of people surviving

```{r}
test_truth <- data_test[["no_show"]]
truth_numeric <- ifelse(test_truth == "Yes", 1, 0)
```

```{r}
score_simple <- prediction$Yes

actual_vs_predicted <- data.table(actual = truth_numeric,
                                  predicted = score_simple)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.6, 0.05),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 1) + xlim(0, 1)
```


```{r}
score_sqrt <- prediction_sqrt$Yes

actual_vs_predicted <- data.table(actual = truth_numeric,
                                  predicted = score_sqrt)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.6, 0.05),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 1) + xlim(0, 1)
```


#### How do they compare? Which score(s) can be regarded as well-calibrated probabilites?
