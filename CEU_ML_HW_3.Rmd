---
title: "Machine Learning -  Homework 3"
author: "Tamas Koncz"
date: '2018-02-11'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

```{r, message=FALSE}
library(data.table)
library(ggplot2)
library(gridExtra)
library(caret)
library(rpart)
library(skimr)
library(ROCR)
library(zoo)

library(datasets)
library(MASS)
library(ISLR)
library(NbClust)
library(factoextra)

options(scipen = 999)

theme_set(theme_bw())   # globally set ggplot theme

set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
```

### 1. PCA for supervised learning (6 points)

```{r}
data <- data.table(Boston)

strCRIM <- "per capita crime rate by town"
strZN <- "proportion of residential land zoned for lots over 25,000 sq.ft."
strINDUS <- "proportion of non-retail business acres per town."
strCHAS <- "Charles River dummy variable (1 if tract bounds river; 0 otherwise)"
strNOX <- "nitric oxides concentration (parts per 10 million)"
strRM <- "average number of rooms per dwelling"
strAGE <- "proportion of owner-occupied units built prior to 1940"
strDIS <- "weighted distances to five Boston employment centres"
strRAD <- "index of accessibility to radial highways"
strTAX <- "full-value property-tax rate per $10,000"
strPTRATIO <- "pupil-teacher ratio by town"
strB <- "1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town"
strLSTAT <- "% lower status of the population"
strMEDV <- "Median value of owner-occupied homes in $1000's"
```

##### a. short exploration of data and find possible predictors of the target variable.

```{r}
print(skim(data))
```


##### b. Create a training and a test set of 50%.
```{r}
training_ratio <- 0.5 

set.seed(93)
train_indices <- createDataPartition(y = data[["crim"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

##### c. Use a linear regression to predict crim and use 10-fold cross validation to assess the predictive power.
```{r}
train_control <- trainControl(method = "cv",
                              number = 10)
set.seed(93)
lm_fit <- train(crim ~ .,
                   method = "lm",
                   data = data_train,
                   trControl = train_control)

mean(data.table(lm_fit$results["RMSE"])$RMSE)
#TODO: plot 10 cases + avg line
```


##### d. Try to improve the model by using PCA for dimensionality reduction. Center and scale your variables and use pcr to conduct a search for the optimal number of principal components. Does PCA improve the fit over the simple linear model?

```{r}
tune_grid <- data.frame(ncomp = 1:13)

set.seed(93)
pcr_fit <- train(crim ~ . , 
                data = data_train, 
                method = "pcr", 
                trControl = train_control,
                tuneGrid = tune_grid,
                preProcess = c("center", "scale")
                )
pcr_fit

#mean(data.table(pcr_fit$resample["RMSE"])$RMSE)
#TODO: plot 10 cases + avg line
```

##### e. Use penalized linear models for the same task. Make sure to include ridge (alpha = 0) to your tune grid. How does the best model compare to that found in d)? Would pre-processing via PCA help this model? (add pca to preProcess). Why do you think the answer can be expected?

```{r}
tune_grid = expand.grid(
              .alpha=0,
              .lambda=seq(0, 2, by = 0.01))

set.seed(93)
ridge_fit <- train(crim ~ .,
                   method = "glmnet",
                   data = data_train,
                   trControl = train_control,
                    tuneGrid = tune_grid,
                    preProcess = c("center", "scale"))

mean(data.table(ridge_fit$results["RMSE"])$RMSE)
#TODO: plot 10 cases + avg line
```

```{r}
set.seed(93)
ridge_pca_fit <- train(crim ~ .,
                   method = "glmnet",
                   data = data_train,
                   trControl = train_control,
                    tuneGrid = tune_grid,
                    preProcess = c("center", "scale", "pca"))

mean(data.table(ridge_pca_fit$results["RMSE"])$RMSE)
#TODO: plot 10 cases + avg line
```


##### f. Evaluate your preferred model on the test set.

```{r}
pred <- predict(ridge_fit, newdata = data_test)
RMSE(pred, data_test$crim)
```

### 2. Clustering on the USArrests dataset (5 points)

```{r}
data <- USArrests
data <- data.table(data, keep.rownames = TRUE)
setnames(data, "rn", "state")
print(skim(data))
```

##### a. Determine the optimal number of clusters as indicated by NbClust heuristics.

Given our variables are not all measured on the same scale, first step is to standardize them, otherwise ones with higher 'abosolute' variance may get overweighted by the algorithm.
Then we can move onto finding the ideal number of cluster with the NbClust method:

```{r, fig.height= 4, fig.align= 'center'}
data_features <- data[ ,.(Assault = scale(Assault, scale = TRUE, center = TRUE),
                          UrbanPop = scale(UrbanPop, scale = TRUE, center = TRUE),
                          Murder = scale(Murder, scale = TRUE, center = TRUE),
                          Rape = scale(Rape, scale = TRUE, center = TRUE)
                          )]

nb <- NbClust(data_features, method = "kmeans", 
              min.nc = 2, max.nc = 10, index = "alllong")
```


```{r, fig.height= 4, fig.width = 6, fig.align= 'center'}
fviz_nbclust(nb) + theme_minimal()
```

According to the majority rule, as well as based on the D index values, the best choice for number of clusters is 2.

##### b. Use the k-means method to cluster states using the number of clusters found in a) and anything else that you think that makes sense. Plot observations colored by clusters in the space of urban population and another (crime-related) variable. (See example code from class, use  factor(km$cluster) to create a vector of class labels).

Running kmeans (k=2) on the already normalized variable set, with 25 iterations:
```{r}
km <- kmeans(data_features, centers = 2, nstart= 25)
```

Let's see the selected cluster centers:

```{r}
print(km$centers)
```

And the distribtion of cases in each cluster:

```{r}
print(table(km$cluster))
```


```{r, fig.width= 15}
data_w_clusters <- cbind(data_features, 
                         data.table("cluster" = factor(km$cluster)))

centers <- data.table(km$centers)


p1 <- ggplot() + 
  geom_point(data= data_w_clusters, 
       aes(x = UrbanPop, 
           y = Assault, 
           color = cluster), size = 2) +
  geom_point(data= centers, 
       aes(x = UrbanPop, 
           y = Assault), size = 5, color = "purple") +
  scale_size(guide = 'none') + 
  scale_color_discrete(guide = 'none')

p2 <- ggplot() + 
  geom_point(data= data_w_clusters, 
       aes(x = UrbanPop, 
           y = Murder, 
           color = cluster), size = 2) +
  geom_point(data= centers, 
       aes(x = UrbanPop, 
           y = Murder), size = 5, color = "purple") +
  scale_size(guide = 'none') +
  scale_color_discrete(guide = 'none')

p3 <- ggplot() + 
  geom_point(data= data_w_clusters, 
       aes(x = UrbanPop, 
           y = Rape, 
           color = cluster), size = 2) +
  geom_point(data= centers, 
       aes(x = UrbanPop, 
           y = Rape), size = 5, color = "purple") +
  scale_size(guide = 'none')

grid.arrange(p1, p2, p3, ncol = 3)
```


##### c. Perform PCA and get the first two principal component coordinates for all observations by

```{r}
#pca_result <- prcomp(data, scale. = TRUE)
#first_two_pc <- data.table(pca_result$x[, 1:2])
```

##### Plot clusters in the coordinate system defined by the first two principal components. How do clusters relate to these?
