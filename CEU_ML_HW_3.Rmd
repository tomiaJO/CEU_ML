---
title: "Machine Learning -  Homework 3"
author: "Tamas Koncz"
date: '2018-02-11'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

```{r, message=FALSE}
library(data.table)
library(ggplot2)
library(gridExtra)
library(caret)
library(rpart)
library(skimr)
library(ROCR)
library(zoo)

library(datasets)
library(MASS)
library(ISLR)
library(NbClust)
library(factoextra)

options(scipen = 999)

theme_set(theme_bw())   # globally set ggplot theme

set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
```

### 1. PCA for supervised learning (6 points)

```{r}
data <- data.table(Boston)

strCRIM <- "per capita crime rate by town"
strZN <- "proportion of residential land zoned for lots over 25,000 sq.ft."
strINDUS <- "proportion of non-retail business acres per town."
strCHAS <- "Charles River dummy variable (1 if tract bounds river; 0 otherwise)"
strNOX <- "nitric oxides concentration (parts per 10 million)"
strRM <- "average number of rooms per dwelling"
strAGE <- "proportion of owner-occupied units built prior to 1940"
strDIS <- "weighted distances to five Boston employment centres"
strRAD <- "index of accessibility to radial highways"
strTAX <- "full-value property-tax rate per $10,000"
strPTRATIO <- "pupil-teacher ratio by town"
strB <- "1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town"
strLSTAT <- "% lower status of the population"
strMEDV <- "Median value of owner-occupied homes in $1000's"
```

##### a. short exploration of data and find possible predictors of the target variable.

```{r}
print(skim(data))
```

```{r, fig.width=5, fig.height=3, fig.align='center'}
ggplot(data, aes(x= crim)) + geom_histogram()
```

##### b. Create a training and a test set of 50%.
```{r}
training_ratio <- 0.5 

set.seed(93)
train_indices <- createDataPartition(y = data[["crim"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

##### c. Use a linear regression to predict crim and use 10-fold cross validation to assess the predictive power.
```{r}
train_control <- trainControl(method = "cv",
                              number = 10)
set.seed(93)
lm_fit <- train(crim ~ .,
                   method = "lm",
                   data = data_train,
                   trControl = train_control)

print(mean(data.table(lm_fit$results["RMSE"])$RMSE))
```


##### d. Try to improve the model by using PCA for dimensionality reduction. Center and scale your variables and use pcr to conduct a search for the optimal number of principal components. Does PCA improve the fit over the simple linear model?

```{r}
tune_grid <- data.frame(ncomp = 1:13)

set.seed(93)
pcr_fit <- train(crim ~ . , 
                data = data_train, 
                method = "pcr", 
                trControl = train_control,
                tuneGrid = tune_grid,
                preProcess = c("center", "scale")
                )

print(mean(data.table(pcr_fit$resample["RMSE"])$RMSE))
```

The fit (measured by avg. RMSE) remains the same - this is due to the fact that the final model is virtually the same, as Caret find the optimal fit with 13 components, 1 for each variable. Using less components would increase the error rate.

##### e. Use penalized linear models for the same task. Make sure to include ridge (alpha = 0) to your tune grid. How does the best model compare to that found in d)? Would pre-processing via PCA help this model? (add pca to preProcess). Why do you think the answer can be expected?

```{r}
tune_grid = expand.grid(
              .alpha=0,
              .lambda=seq(0, 2, by = 0.01))

set.seed(93)
ridge_fit <- train(crim ~ .,
                   method = "glmnet",
                   data = data_train,
                   trControl = train_control,
                    tuneGrid = tune_grid,
                    preProcess = c("center", "scale"))

print(mean(data.table(ridge_fit$results["RMSE"])$RMSE))
```

There is a slight improvement, which can be attributed to the penalty term applied to avoid overfitting.

```{r}
set.seed(93)
ridge_pca_fit <- train(crim ~ .,
                   method = "glmnet",
                   data = data_train,
                   trControl = train_control,
                    tuneGrid = tune_grid,
                    preProcess = c("center", "scale", "pca"))

print(mean(data.table(ridge_pca_fit$results["RMSE"])$RMSE))
```

The model does worse by a thin margin. PCA helps to reduce multicollinearity in the dataset by shrinking features, while ridge will penalize the smallest-variance feature.   

##### f. Evaluate your preferred model on the test set.

```{r}
pred <- predict(ridge_fit, newdata = data_test)
RMSE(pred, data_test$crim)
```

The error rate is ~2x the mean value of the crim variable - this could be considered large.
However, let's remember that the data is heavily skewed - given there were no transformation performed (e.g. log), we can't expect the linear regression to well fit for all ranges in the data.  


### 2. Clustering on the USArrests dataset (5 points)

```{r}
data <- USArrests
data <- data.table(data, keep.rownames = TRUE)
setnames(data, "rn", "state")
print(skim(data))
```

##### a. Determine the optimal number of clusters as indicated by NbClust heuristics.

Given our variables are not all measured on the same scale, first step is to standardize them, otherwise ones with higher 'abosolute' variance may get overweighted by the algorithm.
Then we can move onto finding the ideal number of cluster with the NbClust method:

```{r, fig.height= 4, fig.align= 'center'}
data_features <- data[ ,.(Assault = scale(Assault, scale = TRUE, center = TRUE),
                          UrbanPop = scale(UrbanPop, scale = TRUE, center = TRUE),
                          Murder = scale(Murder, scale = TRUE, center = TRUE),
                          Rape = scale(Rape, scale = TRUE, center = TRUE)
                          )]

nb <- NbClust(data_features, method = "kmeans", 
              min.nc = 2, max.nc = 10, index = "alllong")
```


```{r, fig.height= 4, fig.width = 6, fig.align= 'center'}
fviz_nbclust(nb) + theme_minimal()
```

According to the majority rule, as well as based on the D index values, the best choice for number of clusters is 2.

##### b. Use the k-means method to cluster states using the number of clusters found in a) and anything else that you think that makes sense. Plot observations colored by clusters in the space of urban population and another (crime-related) variable. (See example code from class, use  factor(km$cluster) to create a vector of class labels).

Running kmeans (k=2) on the already normalized variable set, with 25 iterations:
```{r}
km <- kmeans(data_features, centers = 2, nstart= 25)
```

Let's see the selected cluster centers:

```{r}
print(km$centers)
```

And the distribtion of cases in each cluster:

```{r}
print(table(km$cluster))
```


```{r, fig.width= 15}
data_w_clusters <- cbind(data_features, 
                         data.table("cluster" = factor(km$cluster)))

centers <- data.table(km$centers)


p1 <- ggplot() + 
  geom_point(data= data_w_clusters, 
       aes(x = UrbanPop, 
           y = Assault, 
           color = cluster), size = 2) +
  geom_point(data= centers, 
       aes(x = UrbanPop, 
           y = Assault), size = 5, color = "purple") +
  scale_size(guide = 'none') + 
  scale_color_discrete(guide = 'none')

p2 <- ggplot() + 
  geom_point(data= data_w_clusters, 
       aes(x = UrbanPop, 
           y = Murder, 
           color = cluster), size = 2) +
  geom_point(data= centers, 
       aes(x = UrbanPop, 
           y = Murder), size = 5, color = "purple") +
  scale_size(guide = 'none') +
  scale_color_discrete(guide = 'none')

p3 <- ggplot() + 
  geom_point(data= data_w_clusters, 
       aes(x = UrbanPop, 
           y = Rape, 
           color = cluster), size = 2) +
  geom_point(data= centers, 
       aes(x = UrbanPop, 
           y = Rape), size = 5, color = "purple") +
  scale_size(guide = 'none')

grid.arrange(p1, p2, p3, ncol = 3)
```


##### c. Perform PCA and get the first two principal component coordinates for all observations by

```{r}
pca_result <- prcomp(data_features) #scaling was performed already earlier
first_two_pc <- data.table(pca_result$x[, 1:2])
```

##### Plot clusters in the coordinate system defined by the first two principal components. How do clusters relate to these?

```{r, fig.width= 5, fig.height=4, fig.align='center'}
data <- cbind(data, first_two_pc)
data <- cbind(data, data.table("cluster" = factor(km$cluster)))

ggplot() + 
  geom_point(data= data, 
       aes(x = PC1, 
           y = PC2, 
           color = cluster), size = 2) +
  scale_size(guide = 'none')
```

We can observe that the data is nicely separated by PC1 - PC2 has not much differenciating power to separate the clusters.
